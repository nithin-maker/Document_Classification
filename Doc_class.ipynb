{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your data directory\n",
    "data_dir = 'G:/data/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# os.chdir('G:/data/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Balance Sheets', 'Cash Flow', 'Income Statement', 'Notes', 'Others']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the output data\n",
    "output_data = []\n",
    "\n",
    "# Define the Tesseract executable path if needed (depends on your installation)\n",
    "# For example: pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Function to extract text from HTML content\n",
    "def extract_text_from_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        return text\n",
    "def filter_alphabetic(text):\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = [' '.join(re.findall(r'[a-zA-Z]+', line)) for line in lines]\n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "# Traverse the data directory\n",
    "for folder_name in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for html_file in os.listdir(folder_path):\n",
    "            if html_file.endswith('.html'):\n",
    "                file_path = os.path.join(folder_path, html_file)\n",
    "                text = extract_text_from_html(file_path)\n",
    "                filtered_text = filter_alphabetic(text)\n",
    "                # Append the data to the output list\n",
    "                output_data.append({'docno': html_file, 'Text': filtered_text, 'Doc_target': folder_name})\n",
    "\n",
    "# Create a DataFrame from the output data\n",
    "df = pd.DataFrame(output_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docno</th>\n",
       "      <th>Text</th>\n",
       "      <th>Doc_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18320959_3.html</td>\n",
       "      <td>\\n\\n\\n\\n\\nin lacs\\nin lacs\\n\\n\\nStandalone\\nCo...</td>\n",
       "      <td>Balance Sheets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18391125_2.html</td>\n",
       "      <td>\\n\\n\\n\\n\\nConsolidated\\nStandalone\\n\\nParticul...</td>\n",
       "      <td>Balance Sheets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18442877_5.html</td>\n",
       "      <td>\\n\\n\\n\\n\\nStandalone\\nConsolidated\\n\\n\\nAudite...</td>\n",
       "      <td>Balance Sheets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18445487_2.html</td>\n",
       "      <td>\\n\\n\\n\\nGUJARAT NARMADA VALLEY FERTILIZERS CHE...</td>\n",
       "      <td>Balance Sheets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18445494_3.html</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nStandalone\\nConsolidated\\n\\n\\n\\n\\n...</td>\n",
       "      <td>Balance Sheets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             docno                                               Text  \\\n",
       "0  18320959_3.html  \\n\\n\\n\\n\\nin lacs\\nin lacs\\n\\n\\nStandalone\\nCo...   \n",
       "1  18391125_2.html  \\n\\n\\n\\n\\nConsolidated\\nStandalone\\n\\nParticul...   \n",
       "2  18442877_5.html  \\n\\n\\n\\n\\nStandalone\\nConsolidated\\n\\n\\nAudite...   \n",
       "3  18445487_2.html  \\n\\n\\n\\nGUJARAT NARMADA VALLEY FERTILIZERS CHE...   \n",
       "4  18445494_3.html  \\n\\n\\n\\n\\n\\nStandalone\\nConsolidated\\n\\n\\n\\n\\n...   \n",
       "\n",
       "       Doc_target  \n",
       "0  Balance Sheets  \n",
       "1  Balance Sheets  \n",
       "2  Balance Sheets  \n",
       "3  Balance Sheets  \n",
       "4  Balance Sheets  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Particulars\n",
      "As at\n",
      "As ai\n",
      "\n",
      "\n",
      "Audited\n",
      "Audited\n",
      "\n",
      "ASSETS\n",
      "\n",
      "\n",
      "\n",
      "Non Current Assets\n",
      "\n",
      "\n",
      "\n",
      "Property Plant and Equipment\n",
      "\n",
      "\n",
      "\n",
      "Capital Work in progress\n",
      "\n",
      "\n",
      "\n",
      "Investment Property\n",
      "\n",
      "\n",
      "\n",
      "Other Intangible Assets\n",
      "\n",
      "\n",
      "\n",
      "Financial Assets\n",
      "\n",
      "\n",
      "\n",
      "lnve tiT erMs\n",
      "\n",
      "\n",
      "\n",
      "Other Financial Assets\n",
      "\n",
      "\n",
      "\n",
      "Loans\n",
      "\n",
      "\n",
      "\n",
      "Other Non Current Assets\n",
      "\n",
      "\n",
      "\n",
      "Total Non Current Assets\n",
      "\n",
      "\n",
      "\n",
      "Current Assets\n",
      "\n",
      "\n",
      "\n",
      "Inventories\n",
      "\n",
      "\n",
      "\n",
      "Financial Assets\n",
      "\n",
      "\n",
      "\n",
      "Investments\n",
      "\n",
      "\n",
      "\n",
      "Trade Receivables\n",
      "\n",
      "\n",
      "\n",
      "Service Concession Receivable\n",
      "\n",
      "\n",
      "\n",
      "Cash ana Cash Equivalents\n",
      "\n",
      "\n",
      "\n",
      "Bank Balance other than Cash and Cash Equivalents\n",
      "\n",
      "\n",
      "\n",
      "above\n",
      "\n",
      "\n",
      "\n",
      "Othet Financial Assets\n",
      "\n",
      "\n",
      "\n",
      "Loans\n",
      "\n",
      "\n",
      "\n",
      "Other Current Assets\n",
      "\n",
      "\n",
      "\n",
      "Total Current Assets\n",
      "\n",
      "\n",
      "\n",
      "Noo Current Assets Held for sale and Discontinued\n",
      "\n",
      "\n",
      "\n",
      "Operations\n",
      "\n",
      "\n",
      "\n",
      "Total Assets before Regulatory Assets\n",
      "\n",
      "\n",
      "\n",
      "Regulatory deferral account debit balances and related\n",
      "\n",
      "\n",
      "\n",
      "deferred tax balances\n",
      "\n",
      "\n",
      "\n",
      "Total Assets\n",
      "\n",
      "\n",
      "\n",
      "Equity and Liabilities\n",
      "\n",
      "\n",
      "\n",
      "Equity\n",
      "\n",
      "\n",
      "\n",
      "Equity Share Capital\n",
      "\n",
      "\n",
      "\n",
      "Other Equily\n",
      "\n",
      "\n",
      "\n",
      "Total Equity\n",
      "\n",
      "\n",
      "\n",
      "LIABILITIES\n",
      "\n",
      "\n",
      "\n",
      "Non Current Liabilities\n",
      "\n",
      "\n",
      "\n",
      "Financial Liabilities\n",
      "\n",
      "\n",
      "\n",
      "Sorrowings\n",
      "\n",
      "\n",
      "\n",
      "Financiat Lease Obligations\n",
      "\n",
      "\n",
      "\n",
      "Trade Payables\n",
      "\n",
      "\n",
      "\n",
      "Other Financial Liabilities\n",
      "\n",
      "\n",
      "\n",
      "Provisions\n",
      "\n",
      "\n",
      "\n",
      "Deterred Tax Liabilities Net\n",
      "\n",
      "\n",
      "\n",
      "Other Non Current Liabilities\n",
      "\n",
      "\n",
      "\n",
      "Total Non Curront Liabilities\n",
      "\n",
      "\n",
      "\n",
      "Current Liabilities\n",
      "\n",
      "\n",
      "\n",
      "financial Liabilities\n",
      "\n",
      "\n",
      "\n",
      "Borrowings\n",
      "\n",
      "\n",
      "\n",
      "Financial Lease Obligations\n",
      "\n",
      "\n",
      "\n",
      "Trade Payables\n",
      "\n",
      "\n",
      "\n",
      "Other Financial Liabilities\n",
      "\n",
      "\n",
      "\n",
      "Other Current Liabilities\n",
      "\n",
      "\n",
      "\n",
      "Provisions\n",
      "\n",
      "\n",
      "\n",
      "Current Tax Liabilities Net\n",
      "\n",
      "\n",
      "\n",
      "Total Current Liabilities\n",
      "\n",
      "\n",
      "\n",
      "Liabilities of Discontinued Operations\n",
      "\n",
      "\n",
      "\n",
      "Total Equity and Liabilities\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(k.loc[6,'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc_target\n",
       "Others              1224\n",
       "Notes                690\n",
       "Income Statement     305\n",
       "Balance Sheets       270\n",
       "Cash Flow             36\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k['Doc_target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BERT model for sentence embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings using BERT model\n",
    "df['BERT_embeddings'] = df['Text'].apply(lambda x: model.encode(x))\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate TF-IDF embeddings\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(df['Text']).toarray()\n",
    "\n",
    "# Add TF-IDF embeddings to DataFrame\n",
    "df['TFIDF_embeddings'] = list(tfidf_embeddings)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('output_with_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docno</th>\n",
       "      <th>Text</th>\n",
       "      <th>Doc_target</th>\n",
       "      <th>BERT_embeddings</th>\n",
       "      <th>TFIDF_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18320959_3.html</td>\n",
       "      <td>\\n\\n\\n\\n\\nin lacs\\nin lacs\\n\\n\\nStandalone\\nCo...</td>\n",
       "      <td>Balance Sheets</td>\n",
       "      <td>[ 3.18422541e-02 -8.92421678e-02 -5.46203032e-...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18391125_2.html</td>\n",
       "      <td>\\n\\n\\n\\n\\nConsolidated\\nStandalone\\n\\nParticul...</td>\n",
       "      <td>Balance Sheets</td>\n",
       "      <td>[ 2.65763588e-02 -7.69121349e-02 -6.96762428e-...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18442877_5.html</td>\n",
       "      <td>\\n\\n\\n\\n\\nStandalone\\nConsolidated\\n\\n\\nAudite...</td>\n",
       "      <td>Balance Sheets</td>\n",
       "      <td>[ 5.35886828e-03 -4.43138964e-02 -4.27106731e-...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18445487_2.html</td>\n",
       "      <td>\\n\\n\\n\\nGUJARAT NARMADA VALLEY FERTILIZERS CHE...</td>\n",
       "      <td>Balance Sheets</td>\n",
       "      <td>[-6.37322618e-03 -2.75479704e-02 -6.68377727e-...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18445494_3.html</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nStandalone\\nConsolidated\\n\\n\\n\\n\\n...</td>\n",
       "      <td>Balance Sheets</td>\n",
       "      <td>[ 1.99885461e-02 -5.37648201e-02 -4.81042638e-...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             docno                                               Text  \\\n",
       "0  18320959_3.html  \\n\\n\\n\\n\\nin lacs\\nin lacs\\n\\n\\nStandalone\\nCo...   \n",
       "1  18391125_2.html  \\n\\n\\n\\n\\nConsolidated\\nStandalone\\n\\nParticul...   \n",
       "2  18442877_5.html  \\n\\n\\n\\n\\nStandalone\\nConsolidated\\n\\n\\nAudite...   \n",
       "3  18445487_2.html  \\n\\n\\n\\nGUJARAT NARMADA VALLEY FERTILIZERS CHE...   \n",
       "4  18445494_3.html  \\n\\n\\n\\n\\n\\nStandalone\\nConsolidated\\n\\n\\n\\n\\n...   \n",
       "\n",
       "       Doc_target                                    BERT_embeddings  \\\n",
       "0  Balance Sheets  [ 3.18422541e-02 -8.92421678e-02 -5.46203032e-...   \n",
       "1  Balance Sheets  [ 2.65763588e-02 -7.69121349e-02 -6.96762428e-...   \n",
       "2  Balance Sheets  [ 5.35886828e-03 -4.43138964e-02 -4.27106731e-...   \n",
       "3  Balance Sheets  [-6.37322618e-03 -2.75479704e-02 -6.68377727e-...   \n",
       "4  Balance Sheets  [ 1.99885461e-02 -5.37648201e-02 -4.81042638e-...   \n",
       "\n",
       "          TFIDF_embeddings  \n",
       "0  [0. 0. 0. ... 0. 0. 0.]  \n",
       "1  [0. 0. 0. ... 0. 0. 0.]  \n",
       "2  [0. 0. 0. ... 0. 0. 0.]  \n",
       "3  [0. 0. 0. ... 0. 0. 0.]  \n",
       "4  [0. 0. 0. ... 0. 0. 0.]  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk = pd.read_csv('output_with_embeddings.csv')\n",
    "kk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "kk.loc[6,'BERT_embeddings']\n",
    "print(kk.loc[6,'TFIDF_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\HP\\Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[55], line 13\u001b[0m\n    df['BERT_embeddings'] = df['BERT_embeddings'].apply(eval)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\HP\\Project\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4915\u001b[0m in \u001b[0;35mapply\u001b[0m\n    ).apply()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\HP\\Project\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m in \u001b[0;35mapply\u001b[0m\n    return self.apply_standard()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\HP\\Project\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m in \u001b[0;35mapply_standard\u001b[0m\n    mapped = obj._map_values(\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\HP\\Project\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m in \u001b[0;35m_map_values\u001b[0m\n    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\HP\\Project\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m in \u001b[0;35mmap_array\u001b[0m\n    return lib.map_infer(values, mapper, convert=convert)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mlib.pyx:2972\u001b[1;36m in \u001b[1;35mpandas._libs.lib.map_infer\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>:1\u001b[1;36m\u001b[0m\n\u001b[1;33m    [ 3.18422541e-02 -8.92421678e-02 -5.46203032e-02 -1.33948606e-02\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the DataFrame with embeddings\n",
    "df = pd.read_csv('output_with_embeddings.csv')\n",
    "df = df[['docno','Text','Doc_target','BERT_embeddings']]\n",
    "# df\n",
    "# Assuming embeddings are stored as strings, convert them back to lists\n",
    "df['BERT_embeddings'] = df['BERT_embeddings'].apply(eval)\n",
    "\n",
    "# Extract features and target\n",
    "X = np.array(df['BERT_embeddings'].tolist())\n",
    "y = df['Doc_target']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a classifier (e.g., Random Forest)\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Save the model if needed\n",
    "import joblib\n",
    "joblib.dump(classifier, 'document_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  Balance Sheets       1.00      0.92      0.96        51\n",
      "       Cash Flow       1.00      0.83      0.91         6\n",
      "Income Statement       1.00      0.88      0.94        66\n",
      "           Notes       0.89      0.83      0.86       123\n",
      "          Others       0.89      0.96      0.92       259\n",
      "\n",
      "        accuracy                           0.91       505\n",
      "       macro avg       0.96      0.88      0.92       505\n",
      "    weighted avg       0.92      0.91      0.91       505\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Define the path to your data directory\n",
    "data_dir = 'G:/data/data'\n",
    "\n",
    "# Initialize an empty list to hold the output data\n",
    "output_data = []\n",
    "\n",
    "# Function to extract text from HTML content\n",
    "def extract_text_from_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        return text\n",
    "\n",
    "# Function to retain only alphabetic characters in each line\n",
    "def filter_alphabetic(text):\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = [' '.join(re.findall(r'[a-zA-Z]+', line)) for line in lines]\n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "# Traverse the data directory\n",
    "for folder_name in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for html_file in os.listdir(folder_path):\n",
    "            if html_file.endswith('.html'):\n",
    "                file_path = os.path.join(folder_path, html_file)\n",
    "                text = extract_text_from_html(file_path)\n",
    "                filtered_text = filter_alphabetic(text)\n",
    "                # Append the data to the output list\n",
    "                output_data.append({'docno': html_file, 'Text': filtered_text, 'Doc_target': folder_name})\n",
    "\n",
    "# Create a DataFrame from the output data\n",
    "df = pd.DataFrame(output_data)\n",
    "\n",
    "# Initialize the BERT model for sentence embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings using BERT model\n",
    "df['BERT_embeddings'] = df['Text'].apply(lambda x: model.encode(x))\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate TF-IDF embeddings\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(df['Text']).toarray()\n",
    "\n",
    "# Add TF-IDF embeddings to DataFrame\n",
    "df['TFIDF_embeddings'] = list(tfidf_embeddings)\n",
    "\n",
    "# Combine BERT and TF-IDF embeddings (concatenate them)\n",
    "combined_embeddings = np.hstack((np.array(df['BERT_embeddings'].tolist()), tfidf_embeddings))\n",
    "\n",
    "# Prepare target labels\n",
    "y = df['Doc_target']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embeddings, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a classifier (e.g., Random Forest)\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Save the model if needed\n",
    "joblib.dump(classifier, 'document_classifier.pkl')\n",
    "\n",
    "# Save the DataFrame with embeddings to a CSV file\n",
    "df.to_csv('output_with_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
